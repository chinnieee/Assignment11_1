{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1: Three stages of ML to build the hypothesis/model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Model Building\n",
    "2.Model Testing\n",
    "3.Apply the model to real data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2:Standard approach to supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Supervised Learning is the one where the data feeded is tagged or labelled to help make decisions\n",
    "> Standard approach of supervised Learning is to split the data into training and test set almost with 67 % and 33%\n",
    "> Train the model with Train set\n",
    "> Test the model with test set\n",
    "> verify the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3: What is training set and Test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally in ML, a set of data is used to discover the underlying relationship and that we achieve thru training set.\n",
    "Training set is the one which we feed to the model to make it learn and understand the relations for deriving the label\n",
    "Test set is the one used to test the hypothesis of the model.Model predicts the labels of the test set and that is compared against the actual label.Based on comparison accuracy is derived.\n",
    "As mentioned above dataset is split with 67% of train and 33% of test proportion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 4:General principle of ensemble method and bagging and boosting in ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General principle of ensemble method is to combine predictions of various models built with a given learning algorithm to improve the accuracy of the model.Ensemble combine several decision tree classifiers to produce better predictive performance than a single decision tree classifier.\n",
    "Bagging:It is used to reduce the variance of a decision tree classifier.From training sample, data is split into subsets of data with replacement randomly.All these subsets are trained individually and the maximum voting of all predictions are taken in case of classification and avergae of all predictions in case of regression.\n",
    "Boosting:In this technique,learners are learned sequentially and early learners will fit the data to models and analyse for errors.At every step goal is to reduce the error generated by prior tree.Labels which are classified incorrectly are given more weightage so that in the next sequential model it gets classified correctly.Hence this process converts weak learning model to better performing model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 5:How to avoid overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting:Generally it occurs when the model describes random error or noise instead of underlying relationship.It arises when model is relatively complex with too any parameters compared to number of observations.\n",
    "Avoid overfitting:\n",
    "1.Cross Validation:This is a powerful technique to avoid overfitting.Take the training and split into mini batches and tune the model.\n",
    "2.Train with more data:This technique may not work always but most of the times it helps in algorithm to read the underlying relation instead of noise.\n",
    "3.Remove features:Highly correlated features can be removed inorder to avoid overfitting.\n",
    "4.Early stopping:When we are itertaively training the algorithm,we can verify the accuracy of the model for each iteration.After a certain point the model accuracy will get saturated and it begins to overfit the data.So we need to stop training once we see  no further improvement.\n",
    "5.Regularization\n",
    "6.Ensembling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
